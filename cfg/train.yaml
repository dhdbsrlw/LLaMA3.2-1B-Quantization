seed: 42
device: cuda

model_path: /nas2/checkpoints/Llama-3.2-1B
# model_path: /nas2/checkpoints/hf_cache_yj/Llama-3.2-1B-Block-Pruning-n3
# model_path: /nas2/checkpoints/hf_cache_yj/Llama-3.2-1B-Block-Pruning-n6
# model_path: /nas2/checkpoints/hf_cache_yj/Llama-3.2-1B-Block-Pruning-n9

tokenizer_path:
output_dir: /nas2/checkpoints/hf_cache_yj/train_original_bsz128_full_loss

# output_dir: /nas2/checkpoints/hf_cache_yj/train_prune_n3_bsz128_full_loss
# output_dir: /nas2/checkpoints/hf_cache_yj/train_prune_n3_bsz128_output_loss

# output_dir: /nas2/checkpoints/hf_cache_yj/train_prune_n6_bsz128_full_loss
# output_dir: /nas2/checkpoints/hf_cache_yj/train_prune_n6_bsz128_output_loss

# output_dir: /nas2/checkpoints/hf_cache_yj/train_prune_n9_bsz128_output_loss


model_type: pretrain           # pretrain | pruneLLM
ckpt:
use_bfloat: False              # store_true
resume_from_checkpoint:
save_lora_merge: True          # store_true


batch_size: 128
micro_batch_size: 64           # effective batch = batch_size with grad accumulation
num_epochs: 2
learning_rate: 1e-4
warmup_steps: 100
group_by_length: False         # faster but unusual loss curve 가능성
max_seq_len: 128


mask_inputs: False             # store_true (mask prefix tokens with -100)
add_eos_token: False           # store_true


lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules: "q_proj,k_proj,v_proj,o_proj,gate_proj,down_proj,up_proj"


logging_steps: 10
eval_steps: 200
save_steps: 200
save_total_limit: 20