seed: 42
device: cuda

model_path: /nas2/checkpoints/Llama-3.2-1B

tokenizer_path: 
output_dir: 
model_type: pretrain           

ckpt:
use_bfloat: False              # store_true
resume_from_checkpoint: 
save_lora_merge: True          # store_true


max_steps: 10000
batch_size: 128
micro_batch_size: 64           # effective batch = batch_size with grad accumulation
learning_rate: 1e-4
warmup_steps: 500 # 100
group_by_length: False        
max_seq_len: 128


mask_inputs: False             # store_true (mask prefix tokens with -100)
add_eos_token: False           # store_true


lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules: "q_proj,k_proj,v_proj,o_proj,gate_proj,down_proj,up_proj"


logging_steps: 10
eval_steps: 1000 # 200
save_steps: 5000
save_total_limit: 20