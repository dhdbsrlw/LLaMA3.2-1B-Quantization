INFO  Saved Quantize Config:                                                                                                               
{                                                                                                                                          
  "bits": 4,                                                                                                                               
  "group_size": 128,                                                                                                                       
  "desc_act": false,                                                                                                                       
  "sym": true,                                                                                                                             
  "lm_head": false,                                                                                                                        
  "quant_method": "gptq",                                                                                                                  
  "checkpoint_format": "gptq",                                                                                                             
  "pack_dtype": "int32",                                                                                                                   
  "meta": {                                                                                                                                
    "quantizer": [                                                                                                                         
      "gptqmodel:5.4.2"                                                                                                                    
    ],                                                                                                                                     
    "uri": "https://github.com/modelcloud/gptqmodel",                                                                                      
    "damp_percent": 0.05,                                                                                                                  
    "damp_auto_increment": 0.01,                                                                                                           
    "static_groups": false,                                                                                                                
    "true_sequential": true,                                                                                                               
    "mse": 0.0,                                                                                                                            
    "gptaq": false,                                                                                                                        
    "gptaq_alpha": 0.25,                                                                                                                   
    "act_group_aware": true                                                                                                                
  },             
  Files in directory:
quant_log.csv
config.json
generation_config.json
quantize_config.json
Content of saved `generation_config.json`:
{
    "_from_model_config": true,
    "bos_token_id": 128000,
    "do_sample": true,
    "eos_token_id": 128001,
    "temperature": 0.6,
    "top_p": 0.9,
    "transformers_version": "4.57.3"
}
Content of saved `config.json`:    
{
    "architectures": [
        "LlamaForCausalLM"
    ],
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 128000,
    "dtype": "bfloat16",
    "eos_token_id": 128001,
    "head_dim": 64,
    "hidden_act": "silu",
    "hidden_size": 2048,
    "initializer_range": 0.02,
    "intermediate_size": 8192,
    "max_position_embeddings": 131072,
    "mlp_bias": false,
    "model_type": "llama",
    "num_attention_heads": 32,
    "num_hidden_layers": 16,
    "num_key_value_heads": 8,
    "pretraining_tp": 1,
    "quantization_config": {
        "bits": 4,
        "checkpoint_format": "gptq",
        "desc_act": false,
        "group_size": 128,
        "lm_head": false,
        "meta": {
            "act_group_aware": true,
            "damp_auto_increment": 0.01,
            "damp_percent": 0.05,
            "gptaq": false,
            "gptaq_alpha": 0.25,
            "mse": 0.0,
            "quantizer": [
                "gptqmodel:5.4.2"
            ],
            "static_groups": false,
            "true_sequential": true,
            "uri": "https://github.com/modelcloud/gptqmodel"
        },
        "pack_dtype": "int32",
        "pack_impl": "cpu",
        "quant_method": "gptq",
        "sym": true
    },
        "rms_norm_eps": 1e-05,
    "rope_scaling": {
        "factor": 32.0,
        "high_freq_factor": 4.0,
        "low_freq_factor": 1.0,
        "original_max_position_embeddings": 8192,
        "rope_type": "llama3"
    },
    "rope_theta": 500000.0,
    "tie_word_embeddings": true,
    "transformers_version": "4.57.3",
    "use_cache": true,
    "vocab_size": 128256
}
}

INFO  Module: Sync model.embed_tokens <- from turtle (Embedding)                                                                           
INFO  Module: Sync lm_head <- from turtle (Linear)                                                                                         
INFO  Module: Re-tied embedding weights on shell model after full sync                                                                    
INFO  Module: Total synced modules: 2                                                                                                      
INFO  Pre-Quantized model size: 2357.14MB, 2.30GB                                                                                          
INFO  Quantized model size: 984.55MB, 0.96GB                                                                                               
INFO  Size difference: 1372.59MB, 1.34GB - 58.23%  

INFO  | Process quant      | 224   | 1.845  | 1.040 | 232.983 | 44.5%  | model.layers.15.mlp.down_proj                     |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Submodule finalize | 224   | 1.452  | 0.512 | 114.778 | 21.9%  | model.layers.15.mlp.up_proj                       |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Finalize pack      | 112   | 0.906  | 0.468 | 52.372  | 10.0%  | model.layers.15.mlp.up_proj [module.pack_block]   |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Finalize create    | 112   | 0.419  | 0.287 | 32.185  | 6.2%   | model.layers.15.mlp.up_proj                       |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Pre-quant forward  | 64    | 0.991  | 0.484 | 30.995  | 5.9%   | model.layers.15:subset4/4                         |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Forward hook       | 14336 | 0.007  | 0.001 | 20.970  | 4.0%   | model.layers.15.mlp.down_proj                     |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Finalize offload   | 112   | 0.103  | 0.139 | 15.528  | 3.0%   | model.layers.15.mlp.up_proj                       |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Capture inputs     | 2     | 11.119 | 6.033 | 12.067  | 2.3%   | cache_inputs:LlamaDecoderLayer                    |              
INFO  +--------------------+-------+--------+-------+---------+--------+---------------------------------------------------+              
INFO  | Model save         | 1     | 8.677  | 8.677 | 8.677   | 1.7%   | /nas2/checkpoints/hf_cache_yj/Llama-3.2-1B-GPTQ-4bit |           
INFO  +--------------------+-------+--------+-------+---------+--------+------------------------------------------------------+           
INFO  | Post-quant replay  | 15    | 0.199  | 0.168 | 2.515   | 0.5%   | model.layers.14:subset4/4                            |           
INFO  +--------------------+-------+--------+-------+---------+--------+------------------------------------------------------+           
INFO  | Process finalize   | 2     | 0.031  | 0.016 | 0.031   | 0.0%   | tp-pre-pad                                           |           
INFO  +--------------------+-------+--------+-------+---------+--------+------------------------------------------------------+  